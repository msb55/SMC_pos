{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "from deslib.static.oracle import Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X118</th>\n",
       "      <th>X119</th>\n",
       "      <th>X120</th>\n",
       "      <th>X121</th>\n",
       "      <th>X122</th>\n",
       "      <th>X123</th>\n",
       "      <th>X124</th>\n",
       "      <th>X125</th>\n",
       "      <th>X126</th>\n",
       "      <th>X127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15596.1621</td>\n",
       "      <td>1.868245</td>\n",
       "      <td>2.371604</td>\n",
       "      <td>2.803678</td>\n",
       "      <td>7.512213</td>\n",
       "      <td>-2.739388</td>\n",
       "      <td>-3.344671</td>\n",
       "      <td>-4.847512</td>\n",
       "      <td>15326.6914</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071137</td>\n",
       "      <td>-3.037772</td>\n",
       "      <td>3037.0390</td>\n",
       "      <td>3.972203</td>\n",
       "      <td>0.527291</td>\n",
       "      <td>0.728443</td>\n",
       "      <td>1.445783</td>\n",
       "      <td>-0.545079</td>\n",
       "      <td>-0.902241</td>\n",
       "      <td>-2.654529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>26402.0704</td>\n",
       "      <td>2.532401</td>\n",
       "      <td>5.411209</td>\n",
       "      <td>6.509906</td>\n",
       "      <td>7.658469</td>\n",
       "      <td>-4.722217</td>\n",
       "      <td>-5.817651</td>\n",
       "      <td>-7.518333</td>\n",
       "      <td>23855.7812</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.530519</td>\n",
       "      <td>-1.994993</td>\n",
       "      <td>4176.4453</td>\n",
       "      <td>4.281373</td>\n",
       "      <td>0.980205</td>\n",
       "      <td>1.628050</td>\n",
       "      <td>1.951172</td>\n",
       "      <td>-0.889333</td>\n",
       "      <td>-1.323505</td>\n",
       "      <td>-1.749225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>42103.5820</td>\n",
       "      <td>3.454189</td>\n",
       "      <td>8.198175</td>\n",
       "      <td>10.508439</td>\n",
       "      <td>11.611003</td>\n",
       "      <td>-7.668313</td>\n",
       "      <td>-9.478675</td>\n",
       "      <td>-12.230939</td>\n",
       "      <td>37562.3008</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.384784</td>\n",
       "      <td>-2.867291</td>\n",
       "      <td>5914.6685</td>\n",
       "      <td>5.396827</td>\n",
       "      <td>1.403973</td>\n",
       "      <td>2.476956</td>\n",
       "      <td>3.039841</td>\n",
       "      <td>-1.334558</td>\n",
       "      <td>-1.993659</td>\n",
       "      <td>-2.348370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>42825.9883</td>\n",
       "      <td>3.451192</td>\n",
       "      <td>12.113940</td>\n",
       "      <td>16.266853</td>\n",
       "      <td>39.910056</td>\n",
       "      <td>-7.849409</td>\n",
       "      <td>-9.689894</td>\n",
       "      <td>-11.921704</td>\n",
       "      <td>38379.0664</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.607199</td>\n",
       "      <td>-3.058086</td>\n",
       "      <td>6147.4744</td>\n",
       "      <td>5.501071</td>\n",
       "      <td>1.981933</td>\n",
       "      <td>3.569823</td>\n",
       "      <td>4.049197</td>\n",
       "      <td>-1.432205</td>\n",
       "      <td>-2.146158</td>\n",
       "      <td>-2.488957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>58151.1757</td>\n",
       "      <td>4.194839</td>\n",
       "      <td>11.455096</td>\n",
       "      <td>15.715298</td>\n",
       "      <td>17.654915</td>\n",
       "      <td>-11.083364</td>\n",
       "      <td>-13.580692</td>\n",
       "      <td>-16.407848</td>\n",
       "      <td>51975.5899</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.594763</td>\n",
       "      <td>-4.181920</td>\n",
       "      <td>8158.6449</td>\n",
       "      <td>7.174334</td>\n",
       "      <td>1.993808</td>\n",
       "      <td>3.829303</td>\n",
       "      <td>4.402448</td>\n",
       "      <td>-1.930107</td>\n",
       "      <td>-2.931265</td>\n",
       "      <td>-4.088756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y          X0        X1         X2         X3         X4         X5  \\\n",
       "0  0  15596.1621  1.868245   2.371604   2.803678   7.512213  -2.739388   \n",
       "1  0  26402.0704  2.532401   5.411209   6.509906   7.658469  -4.722217   \n",
       "2  0  42103.5820  3.454189   8.198175  10.508439  11.611003  -7.668313   \n",
       "3  0  42825.9883  3.451192  12.113940  16.266853  39.910056  -7.849409   \n",
       "4  0  58151.1757  4.194839  11.455096  15.715298  17.654915 -11.083364   \n",
       "\n",
       "          X6         X7          X8  ...      X118      X119       X120  \\\n",
       "0  -3.344671  -4.847512  15326.6914  ... -1.071137 -3.037772  3037.0390   \n",
       "1  -5.817651  -7.518333  23855.7812  ... -1.530519 -1.994993  4176.4453   \n",
       "2  -9.478675 -12.230939  37562.3008  ... -2.384784 -2.867291  5914.6685   \n",
       "3  -9.689894 -11.921704  38379.0664  ... -2.607199 -3.058086  6147.4744   \n",
       "4 -13.580692 -16.407848  51975.5899  ... -3.594763 -4.181920  8158.6449   \n",
       "\n",
       "       X121      X122      X123      X124      X125      X126      X127  \n",
       "0  3.972203  0.527291  0.728443  1.445783 -0.545079 -0.902241 -2.654529  \n",
       "1  4.281373  0.980205  1.628050  1.951172 -0.889333 -1.323505 -1.749225  \n",
       "2  5.396827  1.403973  2.476956  3.039841 -1.334558 -1.993659 -2.348370  \n",
       "3  5.501071  1.981933  3.569823  4.049197 -1.432205 -2.146158 -2.488957  \n",
       "4  7.174334  1.993808  3.829303  4.402448 -1.930107 -2.931265 -4.088756  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_cell(cell_value):\n",
    "    value = str(cell_value).split(\":\")\n",
    "    return value[1] if len(value) > 1 else value[0]\n",
    "\n",
    "gas_n_batch = 10\n",
    "gas_base_batch_name = \"batch{}.dat\"\n",
    "gas_batch_filenames = [gas_base_batch_name.format(i) for i in range(1, gas_n_batch+1)]\n",
    "\n",
    "gas_dataframes = []\n",
    "for filename in gas_batch_filenames:\n",
    "    df = pd.read_csv(\"./databases/gas-sensor-array-drift/\" + filename, header=None, sep=\" \")\n",
    "    gas_dataframes.append(df)\n",
    "\n",
    "gas_data = pd.concat(gas_dataframes, axis=0, ignore_index=True)\n",
    "gas_n_row, gas_n_column = gas_data.shape\n",
    "gas_columns = ['y'] + [\"X{}\".format(i) for i in range(0, gas_n_column-1)]\n",
    "gas_data.columns = gas_columns\n",
    "gas_data = gas_data.applymap(lambda x: parse_cell(x))\n",
    "gas_data = gas_data.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "gas_data[\"y\"] = gas_data[\"y\"].apply(lambda x: x - 1)\n",
    "\n",
    "gas_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(gas_data[\"y\"], bins=20)\n",
    "plt.title(\"Distribuição das classes #1\")\n",
    "plt.ylabel(\"# de instâncias\")\n",
    "plt.xlabel(\"Classe\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "def run_ensemble_method(X, y, ensemble_name, n_pool=10, kfold=5):\n",
    "    print(\"Running {} ensemble. Pool = {}. K-fold = {}\".format(ensemble_name, n_pool, kfold))\n",
    "    ensemble_method = None\n",
    "    \n",
    "    perceptron = CalibratedClassifierCV(Perceptron(random_state=random_state))\n",
    "    \n",
    "    if ensemble_name == \"Bagging\":\n",
    "        ensemble_method = BaggingClassifier(perceptron, n_estimators=n_pool, n_jobs=-1, verbose=0, \n",
    "                                            random_state=random_state)\n",
    "    elif ensemble_name == \"AdaBoost\":\n",
    "        ensemble_method = AdaBoostClassifier(perceptron, n_estimators=n_pool, random_state=random_state)\n",
    "    elif ensemble_name == \"Random Subspace\":\n",
    "        ensemble_method = BaggingClassifier(perceptron, n_estimators=n_pool, bootstrap=False, max_features=0.5, \n",
    "                                            n_jobs=-1, verbose=0, random_state=random_state)\n",
    "    elif ensemble_name == \"RLO\":\n",
    "        ensemble_method = random_linear_oracle(n_value=n_pool)\n",
    "    elif ensemble_name == \"SGH\":\n",
    "        ensemble_method = SGH()\n",
    "    else:\n",
    "        raise Exception(\"Ensemble method not known.\")\n",
    "    \n",
    "    return evaluate_ensemble_method(X, y, ensemble_method, k_fold=5, is_random_subspace=(ensemble_name == \"Random Subspace\"), is_random_oracles=(ensemble_name == \"RLO\"))\n",
    "\n",
    "def evaluate_ensemble_method(X, y, pool_classifiers, k_fold=5, is_random_subspace=False, is_random_oracles=False):\n",
    "    # Scale the variables to have 0 mean and unit variance\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(data=X)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=k_fold, shuffle=True, random_state=random_state) # criacao dos k_fold estratificados\n",
    "    \n",
    "    oracle_scores = []\n",
    "    index = 0\n",
    "    for train_index, test_index in cv.split(X, list(y.values)): # para cada fold\n",
    "        X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index] # separacao entre dados de treinamento e teste\n",
    "        \n",
    "        epoch = {'execution': index}\n",
    "        \n",
    "        start = time.time()\n",
    "        pool_classifiers.fit(X_train.values, y_train.values)\n",
    "        end = time.time()\n",
    "        \n",
    "        oracle = Oracle(pool_classifiers, random_state=random_state)\n",
    "        oracle.fit(X_train, y_train.values)\n",
    "        epoch['predicted'] = oracle.predict(X_test, y_test.values)\n",
    "        epoch['expected'] = y_test.values\n",
    "        \n",
    "#         epoch['ensemble_fit_time'] = (end-start)\n",
    "#         epoch['hyperplans_per_classes'] = pool_classifiers.hyperplan_per_class_\n",
    "        \n",
    "#         start = time.time()\n",
    "#         if is_random_subspace is False and is_random_oracles is False:\n",
    "#             oracle = Oracle(pool_classifiers, random_state=random_state)\n",
    "#             oracle.fit(X_train, y_train.values)\n",
    "#             score = oracle.score(X_test, y_test.values)\n",
    "#         elif is_random_subspace is True:\n",
    "#             score = oracle_for_random_subspace(pool_classifiers, X_test, y_test.values)\n",
    "#         else:\n",
    "#             score = pool_classifiers.oracle(X_test, y_test.values)\n",
    "            \n",
    "#         end = time.time()\n",
    "#         epoch['oracle_fit_time'] = (end-start)\n",
    "#         epoch['oracle_score'] = score\n",
    "        \n",
    "        oracle_scores.append(epoch)\n",
    "        \n",
    "        index = index + 1\n",
    "\n",
    "    return oracle_scores\n",
    "\n",
    "def oracle_for_random_subspace(meta_model, X_test, y_test):\n",
    "    base_models = meta_model.estimators_\n",
    "    base_models_feats = meta_model.estimators_features_\n",
    "\n",
    "    base_models_preds = []\n",
    "    for i in range(len(base_models)):\n",
    "        X_test_subspace = X_test.iloc[:,base_models_feats[i]] #selecting only the columns used for the ith base model.\n",
    "        y_pred = base_models[i].predict(X_test_subspace)\n",
    "        base_models_preds.append(y_pred)\n",
    "\n",
    "    oracle_hits = []\n",
    "    for i in range(len(y_test)):\n",
    "        oracle_hit = 0\n",
    "        for j in range(len(base_models_preds)):\n",
    "            if base_models_preds[j][i] == y_test[i]:\n",
    "                oracle_hit = 1\n",
    "                break\n",
    "        oracle_hits.append(oracle_hit)\n",
    "\n",
    "    oracle_score = np.sum(oracle_hits)/len(oracle_hits)\n",
    "    return oracle_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pools = [i for i in range(10, 110, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_methods = [\"Bagging\", \"AdaBoost\", \"Random Subspace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaseEnsemble\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "def _build_Perceptron(X,y,curr_training_samples,centroids):   \n",
    "    \"\"\"\n",
    "    Calculates the parameters (weight and bias) of the hyperplane placed in the midpoint between the centroids of most distant classes in X[curr_training_samples].\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape = [n_samples, n_features]\n",
    "        The training data.\n",
    "\n",
    "    y : array of shape = [n_samples]\n",
    "        class labels of each example in X.\n",
    "\n",
    "    curr_training_samples : array of shape = [n_samples]\n",
    "        array of ones and zeros ('1','0'), indicating which samples in X are to be used for placing the hyperplane.\n",
    "\n",
    "    centroids : array of shape = [n_classes,n_features]\n",
    "        centroids of each class considering the previous distribution of X[curr_training_samples].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    centroids : array of shape = [n_classes,n_features]\n",
    "        centroids of each class considering the current distribution of X[curr_training_samples].\n",
    "\n",
    "    weights : array of shape = [n_classes,n_features]\n",
    "        weights of the hyperplane.\n",
    "\n",
    "    bias : array of shape = [n_classes,1]\n",
    "        bias of the hyperplane.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    classes = np.unique(y)\n",
    "    n_classes = classes.size\n",
    "    weights =  np.zeros((n_classes,n_features),float)           \n",
    "    bias =  np.zeros((n_classes),float)\n",
    "\n",
    "    # Identify remaining training samples\n",
    "    idx_curr_training_samples = np.where(curr_training_samples>0)   \n",
    "    \n",
    "    # Set of remaining samples \n",
    "    eval_X = X[idx_curr_training_samples[0]]        \n",
    "    eval_y = y[idx_curr_training_samples[0]]    \n",
    "\n",
    "    # Vector indicating the remaining classes in eval_X/eval_y\n",
    "    curr_classes = np.zeros((n_classes),int)        \n",
    "\n",
    "    for i in range(0,n_classes):\n",
    "        # Select instances from a single class\n",
    "        c = eval_X[np.where(classes[i]==eval_y)]    \n",
    "        if c.size:\n",
    "            # Update centroid of class\n",
    "            centroids[i,] = c.mean(axis=0)   \n",
    "            # Indicate its presence       \n",
    "            curr_classes[i] = 1                     \n",
    "\n",
    "    idx_curr_classes = np.where(curr_classes > 0)\n",
    "\n",
    "    if curr_classes.sum() >= 2:      # More than 2 classes remain in eval_X\n",
    "         # Pairwise distance between current classes\n",
    "        dist_classes = squareform(pdist(centroids[idx_curr_classes[0]]))   \n",
    "        np.fill_diagonal(dist_classes,np.inf)\n",
    "\n",
    "        # Identify the two farthest away\n",
    "        closest_dist = np.unravel_index(np.argmin(dist_classes), dist_classes.shape)    \n",
    "\n",
    "        idx_class_1 = idx_curr_classes[0][closest_dist[0]]      \n",
    "        idx_class_2 = idx_curr_classes[0][closest_dist[1]]\n",
    "\n",
    "    else:       # Only one class remains                                        \n",
    "        # Pairwise distance between all classes in the problem\n",
    "        dist_classes = squareform(pdist(centroids))     \n",
    "        np.fill_diagonal(dist_classes,np.inf)\n",
    "\n",
    "        # Remaining class\n",
    "        idx_class_1 = idx_curr_classes[0][0]\n",
    "        # Most distant from class_1            \n",
    "        idx_class_2 = np.argmin(dist_classes[idx_class_1,])     \n",
    "\n",
    "    # Difference vector between selected classes\n",
    "    diff_vec = centroids[idx_class_1,] - centroids[idx_class_2,]       \n",
    "\n",
    "    if not np.any(diff_vec):\n",
    "        #print('Equal classes centroids!')\n",
    "        w_p = 0.01 * np.ones((n_features),float)\n",
    "        w_p = w_p/sqrt(((w_p)**(2)).sum()) \n",
    "    else:\n",
    "\t\t# Normal vector of diff_vec\n",
    "        w_p = diff_vec/sqrt(((diff_vec)**(2)).sum()) \n",
    "           \n",
    "    \n",
    "    theta_p = np.dot(-w_p,(centroids[idx_class_1,] + centroids[idx_class_2,])/2)    \n",
    "\n",
    "    # Weights of linear classifier\n",
    "    weights[idx_class_1,] = w_p     \n",
    "    weights[idx_class_2,] = -w_p    \n",
    "\n",
    "    # Bias of linear classifier\n",
    "    bias[idx_class_1,] = theta_p    \n",
    "    bias[idx_class_2,] = -theta_p\n",
    "\n",
    "    # Return updated centroids, weights and bias of the linear classifier (and which classes were chosen)\n",
    "    return centroids, weights, bias, classes[idx_class_1], classes[idx_class_2]\n",
    "\n",
    "class SGH(BaseEnsemble):\n",
    "    \"\"\"\n",
    "    Self-Generating Hyperplanes (SGH).\n",
    "\n",
    "    Generates a pool of classifiers which guarantees an Oracle accuracy rate of 100% over the training (input) set. \n",
    "    That is, for each instance in the training set, there is at least one classifier in the pool able to correctly label it. \n",
    "    The generated classifiers are always two-class hyperplanes. \n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    L. I. Kuncheva, A theoretical study on six classi\f",
    "er fusion strategies, IEEE Transactions on\n",
    "    Pattern Analysis and Machine Intelligence 24 (2) (2002) 281-286.\n",
    "\n",
    "    M. A. Souza, G. D. Cavalcanti, R. M. Cruz, R. Sabourin, On the characterization of the\n",
    "    oracle for dynamic classi\f",
    "er selection, in: International Joint Conference on Neural Networks,\n",
    "    IEEE, 2017, pp. 332-339.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=SGDClassifier,\n",
    "                 n_estimators=1,\n",
    "                 correct_classif_label = []\n",
    "                 ):\n",
    "\n",
    "        super(SGH, self).__init__(base_estimator=SGDClassifier,\n",
    "                 n_estimators=1)\n",
    "        \n",
    "        # Pool initially empty\n",
    "        self.estimators_ = []\n",
    "        self.hyperplan_per_class_ = []\n",
    "\n",
    "    def fit(self, X, y, included_samples = np.array([])):\n",
    "        \"\"\"\n",
    "        Populates the SHG ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "\n",
    "        y : array of shape = [n_samples]\n",
    "            class labels of each example in X.\n",
    "\n",
    "        included_samples : array of shape = [n_samples]\n",
    "            array of ones and zeros ('1','0'), indicating which samples in X are to be used for training.\n",
    "            If all, leave blank.\n",
    "            \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        check_X_y(X, y)\n",
    "        return self._fit(X, y, included_samples)\n",
    "\n",
    "    def _fit(self, X, y, included_samples):\n",
    "        \n",
    "        # Set base estimator as the Perceptron\n",
    "        self.base_estimator_ = SGDClassifier(loss=\"perceptron\", eta0=1.e-17,max_iter=1, learning_rate=\"constant\", penalty=None, verbose=0)\n",
    "\n",
    "        # If there is no indication of which instances to include in the training, include all\n",
    "        if included_samples.sum() == 0:\n",
    "            included_samples = np.ones((X.shape[0]),int)\n",
    "\n",
    "        # Generate pool\n",
    "        self._generate_pool(X, y,included_samples)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _generate_pool(self,X, y,curr_training_samples):    # Input data and samples included in the training\n",
    "        \"\"\"\n",
    "        Generates the classifiers in the pool of classifiers (\"estimators_\") using the SGH method.\n",
    "\n",
    "        In each iteration of the method, a hyperplane is placed in the midpoint between the controids of the two most distant classes in the training data. \n",
    "        Then, the newly generated classifier is tested over all samples and the ones it correctly labels are removed from the set. \n",
    "        In the following iteration, a new hyperplane is created based on the classes of the remaining samples in the training set. \n",
    "        The method stops when no sample remains in the training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "\n",
    "        y : array of shape = [n_samples]\n",
    "            class labels of each example in X.\n",
    "\n",
    "        curr_training_samples : array of shape = [n_samples]\n",
    "            array of ones and zeros ('1','0'), indicating which samples in X are to be used for training.\n",
    "            If all, leave blank.\n",
    "            \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Labels of the correct classifier for each training sample\n",
    "        corr_classif_lab = np.zeros((n_samples),int)\n",
    "\n",
    "        # Pool size\n",
    "        n_perceptrons = 0\n",
    "\t\t\n",
    "        n_err = 0\n",
    "        max_err = 50\n",
    "\n",
    "        # Problem's classes\n",
    "        classes = np.unique(y)\n",
    "        n_classes = classes.size\n",
    "\n",
    "        # Centroids of each class\n",
    "        centroids = np.zeros((n_classes,n_features),float)\n",
    "\n",
    "        while curr_training_samples.sum() > 0 and n_err < max_err: # While there are still misclassified samples\n",
    "\n",
    "            # Update centroids and obtain the classifier's coefficients\n",
    "            centroids, weights, bias, cls1, cls2 = _build_Perceptron(X,y,curr_training_samples,centroids)\n",
    "            self.hyperplan_per_class_.append((cls1, cls2))\n",
    "            # Generate classifier\n",
    "            curr_perc = self._make_estimator(append=False, random_state=None)\n",
    "\n",
    "            curr_perc.classes_ = classes\n",
    "            curr_perc.fit(X,y)\n",
    "\n",
    "\n",
    "            # Set classifier's weigths and bias\n",
    "            #if n_classes > 2:\n",
    "            curr_perc.coef_ = weights\n",
    "            curr_perc.intercept_ = bias\n",
    "            #else:\n",
    "            #    curr_perc.coef_ = weights[idx_class_1,]\n",
    "            #    curr_perc.intercept_ = bias[idx_class_1,]\n",
    "\n",
    "            # Add classifier to pool\n",
    "            self.estimators_.append(curr_perc)\n",
    "\n",
    "            # Obtain set with instances that weren't correctly classified yet\n",
    "            idx_curr_training_samples = np.where(curr_training_samples>0)\n",
    "            eval_X = X[idx_curr_training_samples[0]]\n",
    "            eval_y = y[idx_curr_training_samples[0]]\n",
    "        \n",
    "            # Evaluate generated classifier over eval_X\n",
    "            out_curr_perc = self.estimators_[n_perceptrons].predict(eval_X)\n",
    "\n",
    "            # Identify correctly classified samples\n",
    "            idx_correct_eval = (out_curr_perc == eval_y).nonzero()\n",
    "\n",
    "            # Exclude correctly classified samples from current training set\n",
    "            curr_training_samples[idx_curr_training_samples[0][idx_correct_eval[0]]] = 0\n",
    "\n",
    "            # Set classifier label for the correctly classified instances\n",
    "            corr_classif_lab[idx_curr_training_samples[0][idx_correct_eval[0]]] = n_perceptrons\n",
    "            # Increase pool size\n",
    "            n_perceptrons += 1\n",
    "            n_err += 1\n",
    "        \n",
    "        # Update pool size\n",
    "        self.n_estimators = n_perceptrons\n",
    "        # Update classifier labels\n",
    "        self.correct_classif_label = corr_classif_lab\n",
    "\n",
    "        return self\n",
    "\n",
    "    # TODO: implement hit rate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SGH ensemble. Pool = 10. K-fold = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'execution': 0,\n",
       "  'predicted': array([0, 0, 0, ..., 3, 3, 3], dtype=int64),\n",
       "  'expected': array([0, 0, 0, ..., 3, 3, 3], dtype=int64)},\n",
       " {'execution': 1,\n",
       "  'predicted': array([0, 0, 0, ..., 3, 3, 4], dtype=int64),\n",
       "  'expected': array([0, 0, 0, ..., 3, 3, 4], dtype=int64)},\n",
       " {'execution': 2,\n",
       "  'predicted': array([0, 0, 0, ..., 4, 4, 3], dtype=int64),\n",
       "  'expected': array([0, 0, 0, ..., 4, 4, 3], dtype=int64)},\n",
       " {'execution': 3,\n",
       "  'predicted': array([0, 0, 0, ..., 3, 4, 3], dtype=int64),\n",
       "  'expected': array([0, 0, 0, ..., 3, 4, 3], dtype=int64)},\n",
       " {'execution': 4,\n",
       "  'predicted': array([0, 0, 0, ..., 3, 3, 4], dtype=int64),\n",
       "  'expected': array([0, 0, 0, ..., 3, 3, 4], dtype=int64)}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "X_gas = gas_data.iloc[:,1:] # separacao de atributos\n",
    "y_gas = gas_data.iloc[:,0] # separacao de classes\n",
    "\n",
    "result = run_ensemble_method(X_gas, y_gas, \"SGH\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-f9a93b92b840>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predicted'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'expected'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    if r['predicted'] == r['expected']:\n",
    "        print(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
